{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Maestría en Inteligencia Artificial Aplicada**\n",
        "\n",
        "## Curso: **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "### Tecnológico de Monterrey\n",
        "\n",
        "### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## Adtividad Semana 5\n",
        "\n",
        "### **Vectores Embebidos Pre-entrenados: Fasttext**"
      ],
      "metadata": {
        "id": "lNl8G3vHkPSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Nombres y matrículas de los integrantes del equipo:**\n",
        "\n",
        "\n",
        "\n",
        "*   Renzo Zagni A01795457\n",
        "*   Angel Iván Ahumada Arguelles A00398508\n",
        "*   Karina Zafra Vallejo A01793979\n",
        "*   Ernesto Zapata Garza A00915255\n",
        "\n"
      ],
      "metadata": {
        "id": "U69mHA6i201G"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCL2p6MA8NuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a4fda5-e46e-465f-bb57-62c5958221ba"
      },
      "source": [
        "# Aquí deberás incluir todas las librerías que requieras durante esta actividad:\n",
        "\n",
        "import os\n",
        "os.environ['USE_CYTHON'] = '0'\n",
        "!pip install Cython\n",
        "!pip install fasttext\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "from nltk.corpus import stopwords, words , wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from collections import Counter\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (3.0.10)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.12.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "fasttext.util.download_model('en', if_exists='ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "e5VX9Q24FuOM",
        "outputId": "c5a58dcf-7864-49c2-c511-65b0d0a63b27"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cc.en.300.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Pregunta - 1:**\n",
        "\n"
      ],
      "metadata": {
        "id": "4c34ZOnna3Gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descarga los 3 archivos de Canvas y genera un nuevo DataFrame de Pandas con ellos.\n",
        "\n",
        "**Llama simplemente \"df\" a dicho DataFrame.**\n",
        "\n",
        "Los archivos los encuentras en Canvas: amazon5.txt, imdb5.txt, yelp5.txt.\n",
        "\n"
      ],
      "metadata": {
        "id": "yeNllxRdmeWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DIR = \"/content/drive/MyDrive/Colab Notebooks/NLP3\"\n",
        "os.chdir(DIR)\n",
        "\n",
        "dfa = pd.read_csv('amazon5.txt', sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
        "dfi = pd.read_csv('imdb5.txt', sep='\\s{2,}(?=\\d)', names=['review','label'], header=None, engine ='python', encoding='utf-8')\n",
        "dfy = pd.read_csv('yelp5.txt', sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
        "\n",
        "df = pd.concat([dfa, dfi, dfy], ignore_index=True)\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ],
      "metadata": {
        "id": "T_lyEFRkxzC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e5e118-ee46-4ed9-fd24-6e5683cb92b3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifiquemos la información del DataFrame:\n",
        "df.info()"
      ],
      "metadata": {
        "id": "3-w1xMLYnm9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f905684e-8752-42b0-cc56-7eae90377a54"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   review  3000 non-null   object\n",
            " 1   label   3000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 47.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Y veamos sus primeros registros:\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "NfVUcYe1nubT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "9e21eb66-8cfa-4ad9-e294-ca356cb818ef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  label\n",
              "0  So there is no way for me to plug it in here i...      0\n",
              "1                        Good case, Excellent value.      1\n",
              "2                             Great for the jawbone.      1\n",
              "3  Tied to charger for conversations lasting more...      0\n",
              "4                                  The mic is great.      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-39096469-af12-4a15-967c-a6dce60f6f83\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39096469-af12-4a15-967c-a6dce60f6f83')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-39096469-af12-4a15-967c-a6dce60f6f83 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-39096469-af12-4a15-967c-a6dce60f6f83');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7c452c9f-b0df-43c7-ae00-e9aa4890de87\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7c452c9f-b0df-43c7-ae00-e9aa4890de87')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7c452c9f-b0df-43c7-ae00-e9aa4890de87 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2982,\n        \"samples\": [\n          \"We've tried to like this place but after 10+ times I think we're done with them.\",\n          \"The best example of how dumb the writing is when it's established that you can turn the zombie-students back into humans by removing a necklace containing a piece of the meteorite.\",\n          \"It was that loud.Glad to say that the Plantronics 510 maintains a flawless connection to my cell and with no static during normal use.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Pregunta - 2:**"
      ],
      "metadata": {
        "id": "MfZZ0stLmWJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realiza el proceso de limpieza.\n",
        "\n",
        "Aplica el preprocesamiento que consideres adecuado, sin embargo, deberás aplicar necesariamente alguna de las técnicas de lematización.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7F6JF5BommZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "# Separamos la información:\n",
        "X = df.review     # Serie de strings\n",
        "Y = df.label      # Serie de enteros 0s y 1s\n",
        "\n",
        "# Como se vio en el ejercicio pasado, tenemos un problema con los renglones 1125\n",
        "# y 1788 donde el comentario no utiliza caracteres alfabeticos (10/10). Si no\n",
        "# hacemos algo al respecto,  cuando solo consideremos caracteres alfabeticos,\n",
        "# estos renglones se convertiran en elementos vacios los que a  su vez\n",
        "# se asociaran a vectores nulos . Esto nos acarreara problemas al momento  de\n",
        "# procesarlos.\n",
        "# Como estamos hablando de ocurrencias muy bajas (2 de 3000) procederemos a\n",
        "# eliminar renglones vacios luego del procesamiento\n",
        "\n",
        "# Consideremos la siguiente lista de palabras asociada a negaciones en inglés:\n",
        "negwords = [ 'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'don', \"don't\", 'couldn',\n",
        "             \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\",\n",
        "             'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn',\n",
        "             \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\",\n",
        "             'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won',\n",
        "             \"won't\", 'wouldn', \"wouldn't\"]\n",
        "mystopwords =  [x for x in  stopwords.words('english') if x not in negwords]\n",
        "\n",
        "nltk_words = set(words.words())\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def clean_tok(doc):\n",
        "  # Caracteres Alfabeticos\n",
        "  # Incluimos los caracteres del español asi como los caracteres ê y å los cuales\n",
        "  # se encuentran en los comentarios y considero importante preservarlos en este\n",
        "  # momento.\n",
        "  wds = re.sub(r'[^a-zA-ZáéíóúüñÁÉÍÓÚÜÑêÊÅå]',r' ' , doc)\n",
        "  #\n",
        "  # Eliminamos palabras con longitud mayor a 1\n",
        "  wds = re.sub(r'\\b\\w\\b', '', wds)\n",
        "  #\n",
        "  # Elimamos espacios en blanco duplicados\n",
        "  wds =  re.sub(r'\\s+', ' ', wds).strip()\n",
        "  #\n",
        "  # Aplicamos tecnica de lemmatizacion utilizando libreria spacy\n",
        "  tokens = [wd.lemma_ for wd in nlp(wds)]\n",
        "  #\n",
        "  # Tokenizamos los strings y convertimos los token a minusculas\n",
        "  tokens = [ token.lower() for token in tokens]\n",
        "  #\n",
        "  # Eliminamos Stopwords\n",
        "  tokens = [token for token in tokens if token not in mystopwords]\n",
        "  #\n",
        "  # Eliminamos caracteres duplicados no validos\n",
        "  tokens = [clean_dup_chars(x) for x in tokens]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "#\n",
        "# Metodo para eliminar caracteres duplicados. Es una funcion agresiva y en\n",
        "# algunos casos no podra eliminar correctamente los caracteres duplicados invalidos\n",
        "# dependiendo de que tan erronea es la palabra\n",
        "#\n",
        "def clean_dup_chars(word):\n",
        "\n",
        " if word not in nltk_words:                                         # No es una palabra valida en ingles\n",
        "   if re.search(r'(.)\\1', word):                                    # Tiene caracteres duplicados\n",
        "     word = re.sub(r'([a-z])\\1', r'\\1', word[::-1], count=1)[::-1]  # Eliminamos un character duplicado\n",
        "     return clean_dup_chars(word)                                   # Repetimos el proceso\n",
        "   else:\n",
        "    return word\n",
        " else:\n",
        "  return word\n",
        "\n",
        "# Aplicamos los metodos de limpieza\n",
        "#\n",
        "Xclean = [clean_tok(x) for x in X ]\n",
        "\n",
        "# Eliminamos los renglones vacios resultado del proceso de limpieza\n",
        "for i, k in enumerate(Xclean):\n",
        "  if len(k) == 0:\n",
        "    del Xclean[i]\n",
        "    del Y[i]\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ],
      "metadata": {
        "id": "SZnZu0JfE09f"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Despleguemos los primeros comentarios después de tu proceso de limpieza:\n",
        "\n",
        "for x in Xclean[0:5]:\n",
        "  print(x)\n"
      ],
      "metadata": {
        "id": "7jlQuoI2o33T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c2a897c-5a20-40f4-96ef-0d287c3f0ad4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['no', 'way', 'plug', 'us', 'unless', 'go', 'converter']\n",
            "['good', 'case', 'excellent', 'value']\n",
            "['great', 'jawbone']\n",
            "['tie', 'charger', 'conversation', 'last', 'minute', 'major', 'problems']\n",
            "['mic', 'great']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Pregunta - 3:**\n"
      ],
      "metadata": {
        "id": "ygchEdcKqIzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Realicemos una partición aleatoria con los mismos porcentajes de la práctica pasada para poder comparar dichos resultados con los de\n",
        "esta actividad, a saber, 70%, 15% y 15%, para entrenamiento, validación y prueba, respectivamente."
      ],
      "metadata": {
        "id": "7wEIOkkl9Dot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ************* Inicia la sección de agregar código:*****************************\n",
        "\n",
        "x_train, x_val_and_test, y_train, y_val_and_test = train_test_split(Xclean, Y, train_size=.70, shuffle=True, random_state=1)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val_and_test, y_val_and_test, test_size=.50, shuffle=True, random_state=17)\n",
        "\n",
        "\n",
        "# *********** Termina la sección de agregar código *************\n",
        "\n",
        "# verificemos las dimensiones obtenidas:\n",
        "print('X,y Train:', len(x_train), len(y_train))\n",
        "print('X,y Val:', len(x_val), len(y_val))\n",
        "print('X,y Test', len(x_test), len(y_test))"
      ],
      "metadata": {
        "id": "b0SAcYdq9X0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57021d10-9b87-4c47-8bdc-0c49f6178f5d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X,y Train: 2098 2098\n",
            "X,y Val: 450 450\n",
            "X,y Test 450 450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Pregunta - 4:**"
      ],
      "metadata": {
        "id": "1qjKoEqiqBN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Construye tu vocabulario a continuación\n"
      ],
      "metadata": {
        "id": "jENsKiN99r3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a.\tUsa el conjunto de entrenamiento para generar tu vocabulario\n",
        "#     con un tamaño que consideres adecuado:\n",
        "\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "midiccionario = Counter()\n",
        "\n",
        "for k in range(len(x_train)):\n",
        "  midiccionario.update(x_train[k])\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ],
      "metadata": {
        "id": "TzJntmLPqPqC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b.\tIndica el tamaño del vocabulario generado.\n",
        "\n",
        "print('Longitud del vocabulario generado:')\n",
        "\n",
        "\n",
        "# ******* Inicia la sección de agregar código: ***********\n",
        "\n",
        "\n",
        "len(midiccionario)\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ],
      "metadata": {
        "id": "yTDZ0Rr86CUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f87777-9b4d-4a5d-b275-2cebf1e182df"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del vocabulario generado:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3332"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c.\t¿Por qué debe usarse solamente el conjunto de entrenamiento para generar el vocabulario?\n",
        "\n",
        "\n",
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "El diccionario debe generase únicamente con los datos de entrenamientopara evitar el filtrado de información (data leakage) ya que si se utiliza el conjunto de validación y/o de prueba para construir el vocabulario, el modelo se podría estar indirectamente entrenando en estos datos, comprometiendo su imparcialidad.\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++\n"
      ],
      "metadata": {
        "id": "NDa4EhTqrw15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# d.\tCon el vocabulario generado, filtra los conjuntos de entrenamiento,\n",
        "#     validación y prueba para que todos los comentarios usen solamente las\n",
        "#     palabras de este vocabulario.\n",
        "\n",
        "#     Llamar train_x, val_x y test_x a estos tres conjuntos.\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "# Filtramos cada conjunto con las palabras de midiccionario\n",
        "\n",
        "# Como el diccionario for creado a partir del conjunto de entrenamiento, no es\n",
        "# necesario ejecutar el filtrado. Creamos una copia profunda en caso de que\n",
        "# debamos modificar una de las listas sin afectar la otra\n",
        "train_x = x_train[:]\n",
        "\n",
        "val_x = []\n",
        "for ss in x_val:\n",
        "  val_x.append([w for w in ss if w in midiccionario])\n",
        "\n",
        "test_x = []\n",
        "for ss in x_test:\n",
        "  test_x.append([w for w in ss if w in midiccionario])\n",
        "\n",
        "#\n",
        "# Validamos que no tengamos renglones vacios en validacion y prueba\n",
        "\n",
        "for i, k in enumerate(val_x):\n",
        "  if len(k) == 0:\n",
        "    del val_x[i]\n",
        "    y_val.drop(y_val.index[i],inplace=True)\n",
        "\n",
        "for i, k in enumerate(test_x):\n",
        "  if len(k) == 0:\n",
        "    del test_x[i]\n",
        "    y_test.drop(y_test.index[i],inplace=True)\n",
        "\n",
        "print('X,y Train:', len(train_x), len(y_train))\n",
        "print('X,y Val:', len(val_x), len(y_val))\n",
        "print('X,y Test', len(test_x), len(y_test))\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ],
      "metadata": {
        "id": "7ykjxQI3rpxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee01da14-ce2f-449e-82ad-9d2a3cb610c3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X,y Train: 2098 2098\n",
            "X,y Val: 450 450\n",
            "X,y Test 447 447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vemos el resultado de los primeros comentarios del conjunto de entrenamiento:\n",
        "\n",
        "for ss in train_x[0:5]:\n",
        "  print(ss)"
      ],
      "metadata": {
        "id": "iYF2RGuPtQTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1470bfda-877c-4775-b977-63b436130895"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pay', 'bill', 'not', 'tip', 'feel', 'server', 'terrible', 'job']\n",
            "['predictable', 'not', 'bad', 'watch']\n",
            "['love', 'great', 'armband']\n",
            "['watch', 'film', 'want', 'learn', 'work', 'artist']\n",
            "['translate', 'movie', 'living', 'first', 'movie', 'year', 'working', 'experience', 'find', 'offensive', 'intelligence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Pregunta - 5:**"
      ],
      "metadata": {
        "id": "RS0Hxj25vTWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "a. Incluye una tabla comparativa de pros y contras entre los modelos FastText, word2vec de Google y Glove de Stanford."
      ],
      "metadata": {
        "id": "CnHHAza5_P5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "| **Modelo**    | **Pros**                                                                 | **Contras**                                                             |\n",
        "|---------------|--------------------------------------------------------------------------|------------------------------------------------------------------------|\n",
        "| **FastText**  | - Mejor representación de palabras raras.                                | - Mayor consumo de memoria debido a la inclusión de subpalabras.        |\n",
        "|               | - Mejor manejo de errores ortográficos y variaciones en la escritura.    | - Más lento en entrenamiento y predicción comparado con Word2Vec.       |\n",
        "|               | - Representación de palabras mejorada debido a la información de subpalabras.| - Incremento en la complejidad y tamaño del modelo debido a los n-grams.                     |\n",
        "|               |                                                                          |                                                                        |\n",
        "| **Word2Vec**  | - Entrenamiento rápido y eficiente, adecuado para datasets largos.       | - No maneja bien palabras raras o fuera del vocabulario.                |\n",
        "|               | - Captura las relaciones semánticas efectivas.                           | - No considera la estructura interna de las palabras (subpalabras).     |\n",
        "|               | - Menor consumo de memoria comparado con FastText.                       | - Ignora el orden de las palaras .                    |\n",
        "|               | - Uso de modelos de DeepLearning con CBOW y Skip-gram.                   |   |\n",
        "| **GloVe**     | - Captura mejor las estadísticas globales del corpus.                    | - Requiere gran cantidad de memoria para almacenar la matriz de coocurrencia. |\n",
        "|               | - Basado en estadísticas globales de co-ocurrencia, lo que mejora las representaciones. | - Entrenamiento más lento debido al cálculo de la matriz de coocurrencia. |\n",
        "|               | - Resultados consistentes y robustos en varias tareas NLP.               | - Menos flexible en términos de adaptación a datos nuevos y menos efectivo si se tiene un corpora pequeño. |\n",
        "|               | - Representaciones más estables comparado con Word2Vec.                  | - Difícil de actualizar con nuevo vocabulario sin reentrenar desde cero. |\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++\n"
      ],
      "metadata": {
        "id": "uTI9xSgF_Xc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Pregunta - 6:**\n",
        "\n",
        "Utiliza el modelo FastText de vectores embebidos pre-entrenados de dimensión 300 para generar un nuevo diccionario clave-valor, donde la “clave” será cada token o palabra de tu vocabulario y el “valor” será su vector embebido de dimensión 300.\n",
        "\n",
        "Este diccionario deberá ser del mismo tamaño que el vocabulario previo que hayas construido previamente.\n",
        "\n",
        "Es recomendable que una vez que generes el nuevo vocabulario de vectores embebidos, guardes dicho diccionario en un archivo.\n",
        "\n",
        "Recuerda borrar la variable donde descargaste los 2 millones de vectores embebidos Fasttext.\n",
        "\n"
      ],
      "metadata": {
        "id": "ToqRl7fT_fn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "diccionario_npz = 'nlp_semana05.npz'\n",
        "\n",
        "def save_dicc(dicc, filename):\n",
        "    keys = np.array(list(dicc.keys()))\n",
        "    values = np.array(list(dicc.values()))\n",
        "    np.savez(filename, keys=keys, values=values)\n",
        "\n",
        "def load_dicc(filename):\n",
        "    data = np.load(filename)\n",
        "    keys = data['keys']\n",
        "    values = data['values']\n",
        "    dictionary = dict(zip(keys, values))\n",
        "    return dictionary\n",
        "\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n",
        "ft_dim = ft.get_dimension()\n",
        "midicc = {}\n",
        "\n",
        "for k , w in enumerate(midiccionario):\n",
        "  midicc.update({w:ft.get_word_vector(w)})\n",
        "\n",
        "save_dicc(midicc, 'nlp_semana05.npz')\n",
        "del(ft)\n",
        "\n",
        "print(\"Longitud del diccionario                       : \" + str(len(midiccionario)))\n",
        "print(\"Longitud del diccionario con vectores embebidos: \" + str(len(midicc)))\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ],
      "metadata": {
        "id": "UdK-jMfLxHLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f53e0d-481b-461a-9372-51c6b5e4ba73"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del diccionario                       : 3332\n",
            "Longitud del diccionario con vectores embebidos: 3332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuxytFdOnewZ",
        "outputId": "8716ee1e-953d-4840-fc15-74652fba82c1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 18537467\n",
            "-rw------- 1 root root      58226 May 16 21:38 amazon5.txt\n",
            "-rw------- 1 root root 7237176312 May 19 23:42 cc.en.300.bin\n",
            "-rw------- 1 root root 4503593528 May 19 23:42 cc.en.300.bin.gz\n",
            "-rw------- 1 root root          2 May 20 17:27 FastText.joblib\n",
            "-rw------- 1 root root      86022 May 16 21:39 imdb5.txt\n",
            "-rw------- 1 root root    4212154 May 23 03:50 nlp_semana05.npz\n",
            "-rw------- 1 root root 7237176312 May 20 00:18 this.file\n",
            "-rw------- 1 root root      61320 May 16 21:39 yelp5.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Pregunta - 7:**"
      ],
      "metadata": {
        "id": "W4S7q0yR0Mpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Generamos los vectores embebidos a paertir de los conjuntos de entrenamiento, validación y preuba.\n",
        "\n",
        "Los llamaremos trainEmb, valEmb y testEmb, respectivamente."
      ],
      "metadata": {
        "id": "VyeOrkoaC1eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "# Detokenizamos cada renglones\n",
        "train_x_docs = []\n",
        "for k in range(len(train_x)):\n",
        "  train_x_docs.append(' '.join(train_x[k]))\n",
        "\n",
        "val_x_docs = []\n",
        "for k in range(len(val_x)):\n",
        "  val_x_docs.append(' '.join(val_x[k]))\n",
        "\n",
        "test_x_docs = []\n",
        "for k in range(len(test_x)):\n",
        "  test_x_docs.append(' '.join(test_x[k]))\n",
        "\n",
        "#\n",
        "# Generamos las lista de promedio de vectores de los comentarios\n",
        "train_avg_vectors = []\n",
        "for cnt, comment in enumerate(train_x):\n",
        "  comment_vectors = []\n",
        "  for word in comment:\n",
        "    comment_vectors.append(midicc[word])\n",
        "  train_avg_vectors.append([np.mean(x) for x in zip(*comment_vectors)])\n",
        "\n",
        "val_avg_vectors = []\n",
        "for cnt, comment in enumerate(val_x):\n",
        "  comment_vectors = []\n",
        "  for word in comment:\n",
        "    comment_vectors.append(midicc[word])\n",
        "  val_avg_vectors.append([np.mean(x) for x in zip(*comment_vectors)])\n",
        "\n",
        "test_avg_vectors = []\n",
        "for cnt, comment in enumerate(test_x):\n",
        "  comment_vectors = []\n",
        "  for word in comment:\n",
        "    comment_vectors.append(midicc[word])\n",
        "  test_avg_vectors.append([np.mean(x) for x in zip(*comment_vectors)])\n",
        "\n",
        "# Generamos los data frames con los vectores embebidos\n",
        "trainEmb = pd.DataFrame(columns=train_x_docs, data=np.array(train_avg_vectors).T)\n",
        "valEmb = pd.DataFrame(columns=val_x_docs, data=np.array(val_avg_vectors).T)\n",
        "testEmb = pd.DataFrame(columns=test_x_docs, data=np.array(test_avg_vectors).T)"
      ],
      "metadata": {
        "id": "wnfQpkxg0Usq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos las dimensiones de cada conjunto embebido:\n",
        "\n",
        "print(\"Train-Emb:\", trainEmb.shape)\n",
        "print(\"Val-Emb:\", valEmb.shape)\n",
        "print(\"Test-Emb:\", testEmb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJg2jTzBzKXH",
        "outputId": "16de0d02-0df9-4728-a911-be1edacc61b2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train-Emb: (300, 2098)\n",
            "Val-Emb: (300, 450)\n",
            "Test-Emb: (300, 447)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Pregunta - 8:**\n"
      ],
      "metadata": {
        "id": "pibp1LA91CP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Utiliza los modelos de regresión logística y bosque aleatorio (random forest) y encuentra sus desempeños.\n",
        "\n",
        "Compara los resultados con los de la semana anterior."
      ],
      "metadata": {
        "id": "UxC9K0VnGOwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REGRESIÓN LOGÍSTICA:\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "modeloLR = LogisticRegression(C = 0.1,\n",
        "                              random_state = 1)\n",
        "\n",
        "modeloLR.fit(trainEmb.values.T, y_train)\n",
        "\n",
        "print('Logistic Regression: Train-accuracy: %.2f%%' % (100*modeloLR.score(trainEmb.values.T, y_train)))\n",
        "print('Logistic Regression: Val-accuracy: %2.f%%' % (100*modeloLR.score(valEmb.values.T, y_val)))\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ],
      "metadata": {
        "id": "ycwjD8ztGOL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f45ec692-a9a8-4c97-fc7b-3af3a6665bad"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression: Train-accuracy: 82.13%\n",
            "Logistic Regression: Val-accuracy: 82%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BOSQUE ALEATORIO (Random Forest):\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "modeloRF = RandomForestClassifier(n_estimators = 150,\n",
        "                                  min_samples_split = 10,\n",
        "                                  min_samples_leaf = 4,\n",
        "                                  max_leaf_nodes = 20,\n",
        "                                  bootstrap = True,\n",
        "                                  n_jobs = -1,\n",
        "                                  random_state = 1)\n",
        "\n",
        "modeloRF.fit(trainEmb.values.T,y_train)\n",
        "\n",
        "print('Random Forest: Train-accuracy: %.2f%%' % (100*modeloRF.score(trainEmb.values.T, y_train)))\n",
        "print('Random Forest: Val-accuracy: %2.f%%' % (100*modeloRF.score(valEmb.values.T, y_val)))\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ],
      "metadata": {
        "id": "N4n70GHW0sl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6771324d-5b63-4c6a-9382-33bf2366075d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest: Train-accuracy: 89.66%\n",
            "Random Forest: Val-accuracy: 82%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Pregunta - 9:**"
      ],
      "metadata": {
        "id": "WDIiSHvg0_hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Reporte del mejor modelo.\n"
      ],
      "metadata": {
        "id": "dJJtALGZHrGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "# Ambos modelos tienen la misma presicion con el conjunto de validacion pero el modelo de Randon Forest\n",
        "# quedo un poco sobre entrenado por los que decidimos seleccionar el modelo de Regression Logaritmica\n",
        "mejor_modelo = modeloLR\n",
        "\n",
        "print('Test-accuracy con el mejor modelo %.2f%%' % (100*mejor_modelo.score(testEmb.values.T, y_test)))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "pred = mejor_modelo.predict(testEmb.values.T)\n",
        "print('\\nMatriz de confusión con el mejor modelo:')\n",
        "print(confusion_matrix(y_test, pred, labels=[0,1]))\n",
        "\n",
        "print('\\nMatriz de confusión con el mejor modelo en proporciones:')\n",
        "print(confusion_matrix(y_test, pred, labels=[0,1]) / (pred.shape[0] / 100))\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "print()\n",
        "print(\"Accuracy : \" + str(int(accuracy_score(y_test,pred) * 100)) + \"%\")\n",
        "print(\"Precision: \" + str(int(precision_score(y_test,pred) * 100 ))+ \"%\")\n",
        "print(\"Recall   : \" + str(int(recall_score(y_test,pred)* 100))+ \"%\")\n",
        "print(\"F1       : \" + str(int(f1_score(y_test,pred)*100))+ \"%\")\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ],
      "metadata": {
        "id": "ETv4VLjP1GYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e98061-ebb9-40b6-c458-792186a9905d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test-accuracy con el mejor modelo 82.33%\n",
            "\n",
            "Matriz de confusión con el mejor modelo:\n",
            "[[183  35]\n",
            " [ 44 185]]\n",
            "\n",
            "Matriz de confusión con el mejor modelo en proporciones:\n",
            "[[40.93959732  7.82997763]\n",
            " [ 9.84340045 41.38702461]]\n",
            "\n",
            "Accuracy : 82%\n",
            "Precision: 84%\n",
            "Recall   : 80%\n",
            "F1       : 82%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Pregunta - 10:**"
      ],
      "metadata": {
        "id": "YCkh2WfN1MC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Incluye tus comentarios finales de la actividad.\n",
        "\n",
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "En esta asignación, se implementaron vectores embebidos utilizando el modelo FastText para la representación de texto y se entrenaron dos modelos: uno de regresión logística y otro de bosques aleatorios.\n",
        "\n",
        "Los resultados obtenidos fueron los siguientes:\n",
        "\n",
        "- Modelo de Regresión Logística:\n",
        "  - Precisión con datos de entrenamiento: 82.13%\n",
        "  - Precisión con datos de validación: 82%\n",
        "\n",
        "- Modelo de Bosques Aleatorios:**\n",
        "  - Precisión con datos de entrenamiento: 89.66%\n",
        "  - Precisión con datos de validación: 82%\n",
        "\n",
        "El uso de FastText para crear vectores embebidos tiene ventajas importantes sobre los modeles utilizados la semana pasada --conteo de palabras (TF) y TF-IDF--. El modelo FastText contiene vectores pre-entrenados de palabras que facilitan el procesamiento de relaciones y contexto entre las palabras.\n",
        "\n",
        "A diferencia de los métodos basados en conteo o TF-IDF, que solo cuentan la frecuencia de las palabras sin considerar su contexto, FastText tiene en cuenta el contexto de las palabras y utiliza vectores que reflejan mejor las similitudes entre ellas.\n",
        "\n",
        "\n",
        "Al comparar los dos modelos entrenados, la regresión logística demostró ser un modelo robusto con una precisión mas consistente con datos de entrenamiento así como con los datos de validación. Esto es un indicador de que el modelo no quedo sobre-entrenado y que generalizo la data de validación de forma consistente. Por otro lado, aunque el modelos de bosques aleatorios mostró una mayor precisión en los datos de entrenamiento (89%), esta alta precisión no se tradujo en los datos de validación, donde la precisión fue similar a la del modelo de regresión logística (82%). Esto sugiere que los bosques aleatorios se sobre-entreno, capturando o aprendiendo de los patrones específicos del conjunto de entrenamiento que no se generalizan bien a nuevos datos.\n",
        "\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++"
      ],
      "metadata": {
        "id": "4ySFuDQtVuK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Fin de la Actividad de vectores Embebidos - FastText**"
      ],
      "metadata": {
        "id": "bgKHmQTbWJT1"
      }
    }
  ]
}